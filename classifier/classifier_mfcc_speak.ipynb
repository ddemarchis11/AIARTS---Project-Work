{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "from joblib import load\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "HOP_LENGTH = 256         \n",
    "N_FFT = 1024          \n",
    "N_MFCC = 13                 \n",
    "\n",
    "def extract_mfcc(y) -> np.ndarray:\n",
    "    \n",
    "    y, _ = librosa.load(y, sr=SAMPLE_RATE)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        center=True)\n",
    "    delta = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    feats = np.vstack([mfcc, delta, delta2])\n",
    "    return aggregate_stats(feats)\n",
    "\n",
    "def aggregate_stats(feats: np.ndarray) -> np.ndarray:\n",
    "    out = []\n",
    "    for row in feats:\n",
    "        vals = np.asarray(row, dtype=np.float32)\n",
    "        out.extend([\n",
    "            np.mean(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.max(vals) - np.min(vals)\n",
    "        ])\n",
    "    return np.asarray(out, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926cb3a2120f41c791035eaafe5cdf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estrazione MFCC da speak_files:   0%|          | 0/2838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "in_path_music = os.path.abspath(\"mixed_up_data_speak_segmented/speak\")\n",
    "\n",
    "speak_files = [f for f in os.listdir(in_path_music)]\n",
    "\n",
    "df_speak = pd.DataFrame({\n",
    "    \"mfcc_coeff\": [extract_mfcc(os.path.join(in_path_music, f)) for f in tqdm(speak_files, desc=\"Estrazione MFCC da speak_files\")],\n",
    "    \"label\":      1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135d1e50ffaa4079893ea9d3f711858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estrazione MFCC da no_speak_files:   0%|          | 0/2620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_path_noise = os.path.abspath(\"mixed_up_data_speak_segmented/no_speak\")\n",
    "no_speak_files = [f for f in os.listdir(in_path_noise)]\n",
    "\n",
    "df_no_speak = pd.DataFrame({\n",
    "    \"mfcc_coeff\": [extract_mfcc(os.path.join(in_path_noise, f)) for f in tqdm(no_speak_files, desc=\"Estrazione MFCC da no_speak_files\")],\n",
    "    \"label\":      0\n",
    "})\n",
    "\n",
    "train = pd.concat([df_speak, df_no_speak], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine dell'addestramento\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "model = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "\n",
    "X = np.vstack(train[\"mfcc_coeff\"].values) \n",
    "y = train[\"label\"].values                  \n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Fine dell'addestramento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       524\n",
      "           1       0.86      0.87      0.86       568\n",
      "\n",
      "    accuracy                           0.86      1092\n",
      "   macro avg       0.86      0.86      0.86      1092\n",
      "weighted avg       0.86      0.86      0.86      1092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_test/music_pure.wav → no_speak: 0.020, speak: 0.980\n",
      "audio_test/noise_pure.wav → no_speak: 0.867, speak: 0.133\n",
      "audio_test/voice_base_music.wav → no_speak: 0.001, speak: 0.999\n",
      "audio_test/voice_base_pure.wav → no_speak: 0.000, speak: 1.000\n",
      "audio_test/voice_base_noise.wav → no_speak: 0.000, speak: 1.000\n"
     ]
    }
   ],
   "source": [
    "def predict_noisy_probability(wav_path):\n",
    "    emb = extract_mfcc(wav_path).reshape(1, -1)\n",
    "    emb = scaler.transform(emb)\n",
    "    probs = model.predict_proba(emb)[0]   # array di lunghezza 2\n",
    "    return probs\n",
    "\n",
    "test_files = [\n",
    "    \"audio_test/music_pure.wav\",\n",
    "    \"audio_test/noise_pure.wav\",\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "]\n",
    "for test_file in test_files:\n",
    "    if os.path.exists(test_file):\n",
    "        p_no_speak, p_speak = predict_noisy_probability(test_file)\n",
    "        print(f\"{test_file} → no_speak: {p_no_speak:.3f}, speak: {p_speak:.3f}\")\n",
    "    else:\n",
    "        print(f\"File di test non trovato: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve Descrizione\n",
    "Il modello è stato allenato su più dati rispetto alle alternative precedenti, anche perché si era ragionevolmente sicuri funzioansse. Gli MFCC non sono assolutamente adatti a riconoscere sottofondo musicale da noise generico ma sono adatti a distinguere speak/no speak: dimostrato da un'ottima accuracy all'85%, la componente di errore è data principalmente dal fatto che ci potrebbero essere audio music only dove questa, oltre che avere le componenti armoniche alle stesse quefrency udibili in scala Mel del parlato, magari varia tanto rapidamente quanto la voce stessa ed è, quindi, robusta alle first e second derivatives degli MFCC, o meglio, ha lo stesso \"identikit\" della voce. Il modello è allenato su finestre di 3 secondi ed è sempre ragionevolmente sicuro che ci sia voce o meno, come si vede dalla stampa di sopra: l'errore che commette sulla musica pura, viene mitigato segmentando l'audio e prendendo una decisione sulla majority, come si può apprezzare sotto. Questo è interessante, in quanto l'audio preso in considerazione in music_pure ha componente di tono ad alta variazione (assolo di chitarra) solo nella parte finale dell'instrumental, di conseguenza l'armonica a basse frequenza e costante per gran parte della canzone, il che avrebbe \"identikit\" soprattutto di First e Second Derivatives molto diverso dal parlato, viene 'diluito' dalle componenti finali che 'innalzano' Standard Error, Mean... e le altre metriche utilizzate. Ragionando per Majority sulle finestre da 3s, oltre che poter far riferimento a formati su cui sia stato effettivamente allenato il modello, permette di mitigare problemi di questo tipo, anche se, magari, audio con poco parlato, ma pur sempre presente, e molto silenzio/musica vengono etichettati come no_speak, in realtà questo sarebbe anche accettabile per l'utilizzo che vogliamo fare di questo classificatore insomma e perfettamente coerente con la soglia di 85% ottenuta su test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello Logistic Regression salvato in classifier_mfcc_speak.joblib …\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['classifier_mfcc_speak.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_OUT = \"classifier_mfcc_speak.joblib\"\n",
    "\n",
    "print(f\"Modello Logistic Regression salvato in {MODEL_OUT} …\")\n",
    "joblib.dump({'scaler': scaler, 'model': model}, MODEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decisione per audio_test/music_pure (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/noise_pure (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/voice_base_music (majority vote): 1\n",
      "\n",
      "Decisione per audio_test/voice_base_pure (majority vote): 1\n",
      "\n",
      "Decisione per audio_test/voice_base_noise (majority vote): 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "WINDOW_SECONDS = 3\n",
    "\n",
    "def aggregate_stats(feats: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    out = []\n",
    "    for row in feats:\n",
    "        vals = row.astype(np.float32)\n",
    "        out.extend([\n",
    "            np.mean(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.max(vals) - np.min(vals)\n",
    "        ])\n",
    "    return np.asarray(out, dtype=np.float32)\n",
    "\n",
    "def extract_mfcc_from_signal(y: np.ndarray) -> np.ndarray:\n",
    "   \n",
    "    mfcc    = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE,\n",
    "                                   n_mfcc=N_MFCC,\n",
    "                                   n_fft=N_FFT,\n",
    "                                   hop_length=HOP_LENGTH,\n",
    "                                   center=True)\n",
    "    delta1  = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2  = librosa.feature.delta(mfcc, order=2)\n",
    "    feats   = np.vstack([mfcc, delta1, delta2])\n",
    "    return aggregate_stats(feats)\n",
    "\n",
    "def segment_audio(path: str, window_sec: float = WINDOW_SECONDS) -> List[np.ndarray]:\n",
    "\n",
    "    y, _ = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    win_len = int(window_sec * SAMPLE_RATE)\n",
    "    n_segs  = int(np.ceil(len(y) / win_len))\n",
    "    segments = []\n",
    "    for i in range(n_segs):\n",
    "        start = i * win_len\n",
    "        end   = start + win_len\n",
    "        seg   = y[start:end]\n",
    "        if len(seg) < win_len:\n",
    "            seg = np.pad(seg, (0, win_len - len(seg)), mode='constant')\n",
    "        segments.append(seg)\n",
    "    return segments\n",
    "\n",
    "def extract_features_per_segment(path: str) -> np.ndarray:\n",
    "\n",
    "    segments = segment_audio(path)\n",
    "    feats = [extract_mfcc_from_signal(seg) for seg in segments]\n",
    "    return np.vstack(feats)\n",
    "\n",
    "def classify_segments(path: str, model: LogisticRegression, scaler: StandardScaler) -> List[int]:\n",
    "  \n",
    "    X = extract_features_per_segment(path)\n",
    "    X_scaled = scaler.transform(X)\n",
    "        \n",
    "    return model.predict(X_scaled).tolist()\n",
    "\n",
    "def majority_vote(preds: List[int]) -> int:\n",
    "   \n",
    "    cnt = Counter(preds)\n",
    "    return cnt.most_common(1)[0][0]\n",
    "\n",
    "def global_decision_majority(path: str, model: LogisticRegression, scaler: StandardScaler) -> int:\n",
    "\n",
    "    seg_preds = classify_segments(path, model, scaler)\n",
    "    return majority_vote(seg_preds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # questa logica sarà da modificare su ComfyUI per raccogliere input da IO\n",
    "    test_files = [\n",
    "    \"audio_test/music_pure.wav\",\n",
    "    \"audio_test/noise_pure.wav\",\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "    ]\n",
    "    \n",
    "    data = load(MODEL_OUT)\n",
    "    scaler = data['scaler']\n",
    "    model  = data['model']\n",
    "\n",
    "    for audio_file in test_files:\n",
    "        segment_preds = classify_segments(audio_file, model, scaler)\n",
    "\n",
    "        base, _ = os.path.splitext(audio_file)\n",
    "        global_pred = global_decision_majority(audio_file, model, scaler)\n",
    "        print(f\"\\nDecisione per {base} (majority vote): {global_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
